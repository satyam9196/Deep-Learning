# Deep-Learning

# ğŸ¤– Deep Learning Projects

This repository contains hands-on notebooks exploring the **fundamentals of deep learning** using both **PyTorch** and **TensorFlow**. These projects are designed to build a strong understanding of neural networks, activation functions, loss functions, optimizers, and model training.

---

## ğŸ“‚ Project Structure

### ğŸ”¥ PyTorch Fundamentals (`pytorch_fundamentals.ipynb`)
Covers:
- Tensors and basic operations
- Autograd and backpropagation
- Building neural networks with `torch.nn`
- Custom training loops
- Model saving/loading
- GPU acceleration with CUDA

### ğŸ§  TensorFlow Basics (`tensorflow.ipynb`)
Covers:
- TensorFlow and Keras APIs
- Tensor operations
- Building & compiling models
- Model evaluation and tuning
- Working with real datasets
- Callbacks & model checkpoints

---

## ğŸ“š Libraries Used

- `torch`
- `torchvision`
- `tensorflow`
- `keras`
- `matplotlib`, `seaborn`, `pandas`, `numpy` for visualization and data handling

---

## ğŸ§ª Learning Objectives

âœ… Understand the architecture of neural networks  
âœ… Learn how backpropagation and optimizers work  
âœ… Develop models using PyTorch and TensorFlow  
âœ… Visualize training performance with graphs  
âœ… Save and reuse models for inference  

---

## ğŸš€ Getting Started

Install required packages:
```bash
pip install torch torchvision tensorflow matplotlib pandas seaborn
```

Run the notebooks using:
```bash
jupyter notebook
```

---

## ğŸ§‘â€ğŸ’» Ideal For

- Beginners in deep learning  
- Learners switching between PyTorch and TensorFlow  
- Students preparing for AI/ML interviews or certifications

---

## ğŸŒŸ Sample Concepts Covered

- Activation functions: ReLU, Sigmoid, Softmax  
- Loss functions: MSE, Cross-Entropy  
- Optimizers: SGD, Adam  
- Overfitting, Regularization, Dropout  
- GPU vs CPU performance  

---

## ğŸ™Œ Contributions

Pull requests and issue submissions are welcome!

---

## ğŸ‘¨â€ğŸ’» Author

**Satyam9196**  
GitHub: [@satyam9196](https://github.com/satyam9196)

---
